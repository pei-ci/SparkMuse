{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87520192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ldzhangyx.github.io/2020/01/13/music-practice/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e43f8f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lakh Pianoroll Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3d39462",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import clear_output\n",
    "#from ipywidgets import interact, IntSlider\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pypianoroll\n",
    "from pypianoroll import Multitrack, Track, StandardTrack\n",
    "#from tqdm import tqdm\n",
    "#from livelossplot import PlotLosses\n",
    "#from livelossplot.outputs import MatplotlibPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c6178f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "n_tracks = 5  # number of tracks\n",
    "n_pitches = 72  # number of pitches\n",
    "lowest_pitch = 24  # MIDI note number of the lowest pitch\n",
    "n_samples_per_song = 8  # number of samples to extract from each song in the datset\n",
    "n_measures = 4  # number of measures per sample\n",
    "beat_resolution = 4  # temporal resolution of a beat (in timestep)\n",
    "programs = [0, 0, 25, 33, 48]  # program number for each track\n",
    "is_drums = [True, False, False, False, False]  # drum indicator for each track\n",
    "track_names = ['Drums', 'Piano', 'Guitar', 'Bass', 'Strings']  # name of each track\n",
    "tempo = 100\n",
    "\n",
    "# Training\n",
    "batch_size = 16\n",
    "latent_dim = 128\n",
    "n_steps = 6000\n",
    "\n",
    "# Sampling\n",
    "sample_interval = 100  # interval to run the sampler (in step)\n",
    "n_samples = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb865b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "n_tracks = 1  # number of tracks\n",
    "n_pitches = 72  # number of pitches\n",
    "lowest_pitch = 24  # MIDI note number of the lowest pitch\n",
    "n_samples_per_song = 8  # number of samples to extract from each song in the datset\n",
    "n_measures = 4  # number of measures per sample\n",
    "beat_resolution = 4  # temporal resolution of a beat (in timestep)\n",
    "programs = [0]  # program number for each track\n",
    "is_drums = [False]  # drum indicator for each track\n",
    "track_names = ['Piano']  # name of each track\n",
    "tempo = 100\n",
    "\n",
    "# Training\n",
    "batch_size = 16\n",
    "latent_dim = 128\n",
    "n_steps = 9000\n",
    "\n",
    "# Sampling\n",
    "sample_interval = 100  # interval to run the sampler (in step)\n",
    "n_samples = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67904154",
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_resolution = 4 * beat_resolution\n",
    "tempo_array = np.full((4 * 4 * measure_resolution, 1), tempo)\n",
    "assert 24 % beat_resolution == 0, (\n",
    "    \"beat_resolution must be a factor of 24 (the beat resolution used in \"\n",
    "    \"the source dataset).\"\n",
    ")\n",
    "assert len(programs) == len(is_drums) and len(programs) == len(track_names), (\n",
    "    \"Lengths of programs, is_drums and track_names must be the same.\"\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ec47252",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneraterBlock(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, kernel, stride):\n",
    "        super().__init__()\n",
    "        self.transconv = torch.nn.ConvTranspose3d(in_dim, out_dim, kernel, stride)\n",
    "        self.batchnorm = torch.nn.BatchNorm3d(out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.transconv(x)\n",
    "        x = self.batchnorm(x)\n",
    "        return torch.nn.functional.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ceda26f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    \"\"\"A convolutional neural network (CNN) based generator. The generator takes\n",
    "    as input a latent vector and outputs a fake sample.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.transconv0 = GeneraterBlock(latent_dim, 256, (4, 1, 1), (4, 1, 1))\n",
    "        self.transconv1 = GeneraterBlock(256, 128, (1, 4, 1), (1, 4, 1))\n",
    "        self.transconv2 = GeneraterBlock(128, 64, (1, 1, 4), (1, 1, 4))\n",
    "        self.transconv3 = GeneraterBlock(64, 32, (1, 1, 3), (1, 1, 1))\n",
    "        self.transconv4 = torch.nn.ModuleList([\n",
    "            GeneraterBlock(32, 16, (1, 4, 1), (1, 4, 1))\n",
    "            for _ in range(n_tracks)\n",
    "        ])\n",
    "        self.transconv5 = torch.nn.ModuleList([\n",
    "            GeneraterBlock(16, 1, (1, 1, 12), (1, 1, 12))\n",
    "            for _ in range(n_tracks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, latent_dim, 1, 1, 1)\n",
    "        x = self.transconv0(x)\n",
    "        x = self.transconv1(x)\n",
    "        x = self.transconv2(x)\n",
    "        x = self.transconv3(x)\n",
    "        x = [transconv(x) for transconv in self.transconv4]\n",
    "        x = torch.cat([transconv(x_) for x_, transconv in zip(x, self.transconv5)], 1)\n",
    "        x = x.view(-1, n_tracks, n_measures * measure_resolution, n_pitches)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e090bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(torch.nn.Module):\n",
    "    \"\"\"An implementation of Layer normalization that does not require size\n",
    "    information. Copied from https://github.com/pytorch/pytorch/issues/1959.\"\"\"\n",
    "    def __init__(self, n_features, eps=1e-5, affine=True):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.affine = affine\n",
    "        self.eps = eps\n",
    "        if self.affine:\n",
    "            self.gamma = torch.nn.Parameter(torch.Tensor(n_features).uniform_())\n",
    "            self.beta = torch.nn.Parameter(torch.zeros(n_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = [-1] + [1] * (x.dim() - 1)\n",
    "        mean = x.view(x.size(0), -1).mean(1).view(*shape)\n",
    "        std = x.view(x.size(0), -1).std(1).view(*shape)\n",
    "        y = (x - mean) / (std + self.eps)\n",
    "        if self.affine:\n",
    "            shape = [1, -1] + [1] * (x.dim() - 2)\n",
    "            y = self.gamma.view(*shape) * y + self.beta.view(*shape)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f16b569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorBlock(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, kernel, stride):\n",
    "        super().__init__()\n",
    "        self.transconv = torch.nn.Conv3d(in_dim, out_dim, kernel, stride)\n",
    "        self.layernorm = LayerNorm(out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.transconv(x)\n",
    "        x = self.layernorm(x)\n",
    "        return torch.nn.functional.leaky_relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f198183",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    \"\"\"A convolutional neural network (CNN) based discriminator. The\n",
    "    discriminator takes as input either a real sample (in the training data) or\n",
    "    a fake sample (generated by the generator) and outputs a scalar indicating\n",
    "    its authentity.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv0 = torch.nn.ModuleList([\n",
    "            DiscriminatorBlock(1, 16, (1, 1, 12), (1, 1, 12)) for _ in range(n_tracks)\n",
    "        ])\n",
    "        self.conv1 = torch.nn.ModuleList([\n",
    "            DiscriminatorBlock(16, 16, (1, 4, 1), (1, 4, 1)) for _ in range(n_tracks)\n",
    "        ])\n",
    "        self.conv2 = DiscriminatorBlock(16 * n_tracks, 64, (1, 1, 3), (1, 1, 1))\n",
    "        self.conv3 = DiscriminatorBlock(64, 64, (1, 1, 4), (1, 1, 4))\n",
    "        self.conv4 = DiscriminatorBlock(64, 128, (1, 4, 1), (1, 4, 1))\n",
    "        self.conv5 = DiscriminatorBlock(128, 128, (2, 1, 1), (1, 1, 1))\n",
    "        self.conv6 = DiscriminatorBlock(128, 256, (3, 1, 1), (3, 1, 1))\n",
    "        self.dense = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, n_tracks, n_measures, measure_resolution, n_pitches)\n",
    "        x = [conv(x[:, [i]]) for i, conv in enumerate(self.conv0)]\n",
    "        x = torch.cat([conv(x_) for x_, conv in zip(x, self.conv1)], 1)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)          \n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = x.view(-1, 256)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e729db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples):\n",
    "    \"\"\"Compute the gradient penalty for regularization. Intuitively, the\n",
    "    gradient penalty help stablize the magnitude of the gradients that the\n",
    "    discriminator provides to the generator, and thus help stablize the training\n",
    "    of the generator.\"\"\"\n",
    "    # Get random interpolations between real and fake samples\n",
    "    alpha = torch.rand(real_samples.size(0), 1, 1, 1).cuda()\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples))\n",
    "    interpolates = interpolates.requires_grad_(True)\n",
    "    # Get the discriminator output for the interpolations\n",
    "    d_interpolates = discriminator(interpolates)\n",
    "    # Get gradients w.r.t. the interpolations\n",
    "    fake = torch.ones(real_samples.size(0), 1).cuda()\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    # Compute gradient penalty\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d77cc92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in G: 304787\n",
      "Number of parameters in D: 186785\n"
     ]
    }
   ],
   "source": [
    "# Create data loader\n",
    "# data_loader = get_data_loader()\n",
    "PATH = \"modelElse.pt\"\n",
    "checkpoint = torch.load(PATH)\n",
    "# Create neural networks\n",
    "discriminator = Discriminator()\n",
    "generator = Generator()\n",
    "generator.load_state_dict(checkpoint['Generator_state_dict'])\n",
    "discriminator.load_state_dict(checkpoint['Discriminator_state_dict'])\n",
    "print(\"Number of parameters in G: {}\".format(\n",
    "    sum(p.numel() for p in generator.parameters() if p.requires_grad)))\n",
    "print(\"Number of parameters in D: {}\".format(\n",
    "    sum(p.numel() for p in discriminator.parameters() if p.requires_grad)))\n",
    "\n",
    "# Create optimizers\n",
    "d_optimizer = torch.optim.Adam(\n",
    "    discriminator.parameters(), lr=0.001,  betas=(0.5, 0.9))\n",
    "g_optimizer = torch.optim.Adam(\n",
    "    generator.parameters(), lr=0.001, betas=(0.5, 0.9))\n",
    "d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n",
    "g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n",
    "# Prepare the inputs for the sampler, which wil run during the training\n",
    "sample_latent = torch.randn(n_samples, latent_dim)\n",
    "\n",
    "# Transfer the neural nets and samples to GPU\n",
    "if torch.cuda.is_available():\n",
    "    discriminator = discriminator.cuda()\n",
    "    generator = generator.cuda()\n",
    "    sample_latent = sample_latent.cuda()\n",
    "\n",
    "# Create an empty dictionary to sotre history samples\n",
    "history_samples = {}\n",
    "\n",
    "# Create a LiveLoss logger instance for monitoring\n",
    "#liveloss = PlotLosses(outputs=[MatplotlibPlot(cell_size=(6,2))])\n",
    "\n",
    "# Initialize step\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40b80154",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_latent = torch.randn(n_samples, latent_dim)\n",
    "sample_latent = sample_latent.cuda()\n",
    "samples = generator(sample_latent).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8285c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples.transpose(1, 0, 2, 3).reshape(n_tracks, -1, n_pitches)\n",
    "tracks = []\n",
    "for idx, (program, is_drum, track_name) in enumerate(\n",
    "    zip(programs, is_drums, track_names)\n",
    "):\n",
    "    pianoroll = np.pad(\n",
    "        samples[idx] > 0.5,\n",
    "        ((0, 0), (lowest_pitch, 128 - lowest_pitch - n_pitches))\n",
    "    )\n",
    "    tracks.append(\n",
    "        StandardTrack(\n",
    "            name=track_name,\n",
    "            program=program,\n",
    "            is_drum=is_drum,\n",
    "            pianoroll=pianoroll\n",
    "        )\n",
    "    )\n",
    "m = Multitrack(\n",
    "    tracks=tracks,\n",
    "    tempo=tempo_array,\n",
    "    resolution=beat_resolution\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e585e45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pypianoroll.write(\"MuseGanElse2.mid\",m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3093aabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Multitrack(name=None, resolution=4, tempo=array(shape=(256, 1), dtype=float64), tracks=[StandardTrack(name='Drums', program=0, is_drum=True, pianoroll=array(shape=(256, 128), dtype=uint8)), StandardTrack(name='Piano', program=0, is_drum=False, pianoroll=array(shape=(256, 128), dtype=uint8)), StandardTrack(name='Guitar', program=25, is_drum=False, pianoroll=array(shape=(256, 128), dtype=uint8)), StandardTrack(name='Bass', program=33, is_drum=False, pianoroll=array(shape=(256, 128), dtype=uint8)), StandardTrack(name='Strings', program=48, is_drum=False, pianoroll=array(shape=(256, 128), dtype=uint8))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539c5870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
