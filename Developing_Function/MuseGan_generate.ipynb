{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87520192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ldzhangyx.github.io/2020/01/13/music-practice/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e43f8f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lakh Pianoroll Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3d39462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\fluid\\bin\\libfluidsynth-3.dll\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pypianoroll\n",
    "from pypianoroll import Multitrack, Track, StandardTrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c6178f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "n_tracks = 5  # number of tracks\n",
    "n_pitches = 72  # number of pitches\n",
    "lowest_pitch = 24  # MIDI note number of the lowest pitch\n",
    "n_samples_per_song = 8  # number of samples to extract from each song in the datset\n",
    "n_measures = 4  # number of measures per sample\n",
    "beat_resolution = 4  # temporal resolution of a beat (in timestep)\n",
    "programs = [0, 0, 25, 33, 48]  # program number for each track\n",
    "is_drums = [True, False, False, False, False]  # drum indicator for each track\n",
    "track_names = ['Drums', 'Piano', 'Guitar', 'Bass', 'Strings']  # name of each track\n",
    "tempo = 100\n",
    "\n",
    "# Training\n",
    "batch_size = 16\n",
    "latent_dim = 128\n",
    "n_steps = 6000\n",
    "\n",
    "# Sampling\n",
    "sample_interval = 100  # interval to run the sampler (in step)\n",
    "n_samples = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb865b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "n_tracks = 1  # number of tracks\n",
    "n_pitches = 72  # number of pitches\n",
    "lowest_pitch = 24  # MIDI note number of the lowest pitch\n",
    "n_samples_per_song = 8  # number of samples to extract from each song in the datset\n",
    "n_measures = 4  # number of measures per sample\n",
    "beat_resolution = 4  # temporal resolution of a beat (in timestep)\n",
    "programs = [0]  # program number for each track\n",
    "is_drums = [False]  # drum indicator for each track\n",
    "track_names = ['Piano']  # name of each track\n",
    "tempo = 100\n",
    "\n",
    "# Training\n",
    "batch_size = 16\n",
    "latent_dim = 128\n",
    "n_steps = 9000\n",
    "\n",
    "# Sampling\n",
    "sample_interval = 100  # interval to run the sampler (in step)\n",
    "n_samples = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67904154",
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_resolution = 4 * beat_resolution\n",
    "tempo_array = np.full((4 * 4 * measure_resolution, 1), tempo)\n",
    "assert 24 % beat_resolution == 0, (\n",
    "    \"beat_resolution must be a factor of 24 (the beat resolution used in \"\n",
    "    \"the source dataset).\"\n",
    ")\n",
    "assert len(programs) == len(is_drums) and len(programs) == len(track_names), (\n",
    "    \"Lengths of programs, is_drums and track_names must be the same.\"\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ec47252",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneraterBlock(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, kernel, stride):\n",
    "        super().__init__()\n",
    "        self.transconv = torch.nn.ConvTranspose3d(in_dim, out_dim, kernel, stride)\n",
    "        self.batchnorm = torch.nn.BatchNorm3d(out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.transconv(x)\n",
    "        x = self.batchnorm(x)\n",
    "        return torch.nn.functional.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceda26f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    \"\"\"A convolutional neural network (CNN) based generator. The generator takes\n",
    "    as input a latent vector and outputs a fake sample.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.transconv0 = GeneraterBlock(latent_dim, 256, (4, 1, 1), (4, 1, 1))\n",
    "        self.transconv1 = GeneraterBlock(256, 128, (1, 4, 1), (1, 4, 1))\n",
    "        self.transconv2 = GeneraterBlock(128, 64, (1, 1, 4), (1, 1, 4))\n",
    "        self.transconv3 = GeneraterBlock(64, 32, (1, 1, 3), (1, 1, 1))\n",
    "        self.transconv4 = torch.nn.ModuleList([\n",
    "            GeneraterBlock(32, 16, (1, 4, 1), (1, 4, 1))\n",
    "            for _ in range(n_tracks)\n",
    "        ])\n",
    "        self.transconv5 = torch.nn.ModuleList([\n",
    "            GeneraterBlock(16, 1, (1, 1, 12), (1, 1, 12))\n",
    "            for _ in range(n_tracks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, latent_dim, 1, 1, 1)\n",
    "        x = self.transconv0(x)\n",
    "        x = self.transconv1(x)\n",
    "        x = self.transconv2(x)\n",
    "        x = self.transconv3(x)\n",
    "        x = [transconv(x) for transconv in self.transconv4]\n",
    "        x = torch.cat([transconv(x_) for x_, transconv in zip(x, self.transconv5)], 1)\n",
    "        x = x.view(-1, n_tracks, n_measures * measure_resolution, n_pitches)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e090bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(torch.nn.Module):\n",
    "    \"\"\"An implementation of Layer normalization that does not require size\n",
    "    information. Copied from https://github.com/pytorch/pytorch/issues/1959.\"\"\"\n",
    "    def __init__(self, n_features, eps=1e-5, affine=True):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.affine = affine\n",
    "        self.eps = eps\n",
    "        if self.affine:\n",
    "            self.gamma = torch.nn.Parameter(torch.Tensor(n_features).uniform_())\n",
    "            self.beta = torch.nn.Parameter(torch.zeros(n_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = [-1] + [1] * (x.dim() - 1)\n",
    "        mean = x.view(x.size(0), -1).mean(1).view(*shape)\n",
    "        std = x.view(x.size(0), -1).std(1).view(*shape)\n",
    "        y = (x - mean) / (std + self.eps)\n",
    "        if self.affine:\n",
    "            shape = [1, -1] + [1] * (x.dim() - 2)\n",
    "            y = self.gamma.view(*shape) * y + self.beta.view(*shape)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f16b569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorBlock(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, kernel, stride):\n",
    "        super().__init__()\n",
    "        self.transconv = torch.nn.Conv3d(in_dim, out_dim, kernel, stride)\n",
    "        self.layernorm = LayerNorm(out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.transconv(x)\n",
    "        x = self.layernorm(x)\n",
    "        return torch.nn.functional.leaky_relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f198183",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    \"\"\"A convolutional neural network (CNN) based discriminator. The\n",
    "    discriminator takes as input either a real sample (in the training data) or\n",
    "    a fake sample (generated by the generator) and outputs a scalar indicating\n",
    "    its authentity.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv0 = torch.nn.ModuleList([\n",
    "            DiscriminatorBlock(1, 16, (1, 1, 12), (1, 1, 12)) for _ in range(n_tracks)\n",
    "        ])\n",
    "        self.conv1 = torch.nn.ModuleList([\n",
    "            DiscriminatorBlock(16, 16, (1, 4, 1), (1, 4, 1)) for _ in range(n_tracks)\n",
    "        ])\n",
    "        self.conv2 = DiscriminatorBlock(16 * n_tracks, 64, (1, 1, 3), (1, 1, 1))\n",
    "        self.conv3 = DiscriminatorBlock(64, 64, (1, 1, 4), (1, 1, 4))\n",
    "        self.conv4 = DiscriminatorBlock(64, 128, (1, 4, 1), (1, 4, 1))\n",
    "        self.conv5 = DiscriminatorBlock(128, 128, (2, 1, 1), (1, 1, 1))\n",
    "        self.conv6 = DiscriminatorBlock(128, 256, (3, 1, 1), (3, 1, 1))\n",
    "        self.dense = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, n_tracks, n_measures, measure_resolution, n_pitches)\n",
    "        x = [conv(x[:, [i]]) for i, conv in enumerate(self.conv0)]\n",
    "        x = torch.cat([conv(x_) for x_, conv in zip(x, self.conv1)], 1)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)          \n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = x.view(-1, 256)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6882de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tracks = 5\n",
    "latent_dim = 128\n",
    "n_samples = 4\n",
    "n_measures = 4\n",
    "beat_resolution = 4\n",
    "measure_resolution = 4 * beat_resolution\n",
    "n_pitches = 72  # number of pitches\n",
    "\n",
    "programs = [0, 0, 25, 33, 48]\n",
    "is_drums = [True, False, False, False, False]  # drum indicator for each track\n",
    "track_names = ['Drums', 'Piano', 'Guitar', 'Bass', 'Strings']  # name of each track\n",
    "lowest_pitch = 24\n",
    "tempo = 100\n",
    "tempo_array = np.full((4 * 4 * measure_resolution, 1), tempo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d77cc92a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_tracks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(PATH)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Create neural networks\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m Discriminator()\n\u001b[0;32m      7\u001b[0m generator \u001b[38;5;241m=\u001b[39m Generator()\n\u001b[0;32m      8\u001b[0m generator\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerator_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn [6], line 10\u001b[0m, in \u001b[0;36mDiscriminator.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[1;32m---> 10\u001b[0m         DiscriminatorBlock(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m16\u001b[39m, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m12\u001b[39m), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m12\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mn_tracks\u001b[49m)\n\u001b[0;32m     11\u001b[0m     ])\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[0;32m     13\u001b[0m         DiscriminatorBlock(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m16\u001b[39m, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_tracks)\n\u001b[0;32m     14\u001b[0m     ])\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2 \u001b[38;5;241m=\u001b[39m DiscriminatorBlock(\u001b[38;5;241m16\u001b[39m \u001b[38;5;241m*\u001b[39m n_tracks, \u001b[38;5;241m64\u001b[39m, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_tracks' is not defined"
     ]
    }
   ],
   "source": [
    "# Create data loader\n",
    "# data_loader = get_data_loader()\n",
    "PATH = \"Happy_01.pt\"\n",
    "checkpoint = torch.load(PATH)\n",
    "# Create neural networks\n",
    "discriminator = Discriminator()\n",
    "generator = Generator()\n",
    "generator.load_state_dict(checkpoint['Generator_state_dict'])\n",
    "discriminator.load_state_dict(checkpoint['Discriminator_state_dict'])\n",
    "\"\"\"print(\"Number of parameters in G: {}\".format(\n",
    "    sum(p.numel() for p in generator.parameters() if p.requires_grad)))\n",
    "print(\"Number of parameters in D: {}\".format(\n",
    "    sum(p.numel() for p in discriminator.parameters() if p.requires_grad)))\"\"\"\n",
    "\n",
    "# Create optimizers\n",
    "d_optimizer = torch.optim.Adam(\n",
    "    discriminator.parameters(), lr=0.001,  betas=(0.5, 0.9))\n",
    "g_optimizer = torch.optim.Adam(\n",
    "    generator.parameters(), lr=0.001, betas=(0.5, 0.9))\n",
    "d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n",
    "g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n",
    "\n",
    "sample_latent = torch.randn(n_samples, latent_dim)\n",
    "# Transfer the neural nets and samples to GPU\n",
    "if torch.cuda.is_available():\n",
    "    discriminator = discriminator.cuda()\n",
    "    generator = generator.cuda()\n",
    "    sample_latent = sample_latent.cuda()\n",
    "\n",
    "samples = generator(sample_latent).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40b80154",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_latent = torch.randn(n_samples, latent_dim)\n",
    "sample_latent = sample_latent.cuda()\n",
    "samples = generator(sample_latent).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8285c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples.transpose(1, 0, 2, 3).reshape(n_tracks, -1, n_pitches)\n",
    "tracks = []\n",
    "for idx, (program, is_drum, track_name) in enumerate(\n",
    "    zip(programs, is_drums, track_names)\n",
    "):\n",
    "    pianoroll = np.pad(\n",
    "        samples[idx] > 0.5,\n",
    "        ((0, 0), (lowest_pitch, 128 - lowest_pitch - n_pitches))\n",
    "    )\n",
    "    tracks.append(\n",
    "        StandardTrack(\n",
    "            name=track_name,\n",
    "            program=program,\n",
    "            is_drum=is_drum,\n",
    "            pianoroll=pianoroll\n",
    "        )\n",
    "    )\n",
    "m = Multitrack(\n",
    "    tracks=tracks,\n",
    "    tempo=tempo_array,\n",
    "    resolution=beat_resolution\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e585e45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pypianoroll.write(\"test.mid\",m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3093aabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Multitrack(name=None, resolution=4, tempo=array(shape=(256, 1), dtype=float64), tracks=[StandardTrack(name='Drums', program=0, is_drum=True, pianoroll=array(shape=(256, 128), dtype=uint8)), StandardTrack(name='Piano', program=0, is_drum=False, pianoroll=array(shape=(256, 128), dtype=uint8)), StandardTrack(name='Guitar', program=25, is_drum=False, pianoroll=array(shape=(256, 128), dtype=uint8)), StandardTrack(name='Bass', program=33, is_drum=False, pianoroll=array(shape=(256, 128), dtype=uint8)), StandardTrack(name='Strings', program=48, is_drum=False, pianoroll=array(shape=(256, 128), dtype=uint8))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539c5870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
